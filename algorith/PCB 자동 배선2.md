# **DQN + MCTS + PPO 하이브리드 구조**
# **실시간성**, **성능**, **복잡한 회로 대응력**
# **100% 성공률**을 동시에 만족하기 위해 최근 가장 주목받는 접근

---

## 🧭 1. PCB 자동 배선 문제란?

- **목표**: 각 신호망(Net) 내의 핀들을 **전기적, 물리적 제약**을 만족하며 연결
- **과제**:
  - **충돌 없음 (DRC 준수)**
  - **경로 길이 최소화 (HPWL, Euclidean)**
  - **비아(Via) 최소화**
  - **다층 PCB에서 층간 전환 처리**

---

## 🧠 2. 핵심 개념 요약

| 요소         | 설명 |
|--------------|------|
| **DQN**      | 실시간성 & 빠른 추론을 위한 Q-learning 기반 정책 |
| **MCTS**     | 복잡한 탐색 문제 해결용 트리 기반 경로 탐색 알고리즘 |
| **PPO**      | 안정적인 정책 학습 (정책 기반) |
| **하이브리드 구조** | MCTS 탐색 중 PPO 정책을 **롤아웃 가이드**로 활용, DQN은 실시간 결정용 |

---

## ⚙️ 3. 전체 아키텍처 흐름

```plaintext
[Netlist & Layout Grid]
        ↓
   [환경 상태 인코딩] → CNN/GNN 등
        ↓
   [PPO 정책 네트워크] (Soft policy)
        ↓
  PPO 정책이 MCTS 탐색 시 가이드 역할
        ↓
[MCTS] → 여러 경로 후보 탐색 (Backtracking 포함)
        ↓
[최적 경로 결정] → Action 선택
        ↓
DQN이 Q-value 기반으로 즉시 추론 (실시간 대응)
        ↓
[경로 실행 + 보상 피드백] → 환경 업데이트
```

---

## 🧩 4. 각 알고리즘의 역할

### ✅ DQN (Deep Q-Network)

- **장점**: 빠른 추론, 경험 재사용(Replay buffer)
- **용도**:
  - 매 행동마다 가장 Q값이 높은 방향 선택
  - 실시간에서 빠른 의사결정 가능
- **상태 표현**:
  - 12차원 피처 벡터 또는 grid-based 이미지
- **행동 공간**:
  - 격자 기반 상하좌우 + 층 이동 (총 6개)

👉 Liao et al. (2019) 연구에서 DQN 기반 글로벌 라우터가 기존 A* 방식보다 복잡한 회로에서 더 나은 성능을 보임

---

### ✅ MCTS (Monte Carlo Tree Search)

- **장점**: 복잡한 탐색 공간에서 최적화 가능
- **용도**:
  - 여러 경로를 시뮬레이션하고 결과 비교
  - **롤아웃 시 PPO 정책**을 따르며 "좋은 경로"로 유도
- **기여**:
  - RL만 사용할 때 실패하던 경로 탐색 성공
  - 탐색 깊이에 따라 보장된 결과 품질 확보

👉 Bao et al. (2022) 논문에서 PPO+MCTS 구조로 100% 배선 성공률 달성

---

### ✅ PPO (Proximal Policy Optimization)

- **장점**: 학습 안정성 / 롤아웃 정책으로 적합
- **용도**:
  - MCTS가 시뮬레이션할 때 따라가는 **rollout policy**
  - 경험 기반으로 "좋은 행동 방향"을 안내
- **훈련 방식**:
  - 경로 성공률, 경로 길이, 비아 수 등을 보상에 반영하여 학습

👉 RL 단독으로는 어려운 탐색을 가이드하는 **전략적 역할**

---

## 🧪 5. 성능 및 사례

| 논문/시스템 | 구성 | 성능 결과 |
|-------------|------|-----------|
| **MCTS+PPO** (Bao, 2022) | PPO로 학습한 정책 + MCTS로 최적 경로 탐색 | 100% 배선 성공 / 배선 길이 A* 수준 |
| **DQN Global Router** (Liao, 2019) | DQN만으로 글로벌 라우팅 수행 | A* 대비 복잡 보드에서 우수, Q-value 기반 실시간 추론 |
| **Hybrid 구조 제안** | PPO(MCTS 가이드) + DQN(즉시 실행) | 복잡성 + 실시간성 동시에 충족 가능 |

---

## 🧠 상태/행동/보상 예시

| 항목 | 예시 |
|------|------|
| 상태 | 배선 중 위치, 목표 방향, 충돌 여부, 주변 장애물, 이전 행동 |
| 행동 | 상/하/좌/우 이동, 층 위/아래로 전환 (via), 배선 종료 |
| 보상 |  
- 경로 길이 패널티  
- 충돌 발생 시 음의 보상  
- 목표 연결 성공 시 높은 보상  
- 비아 사용 시 소규모 패널티  
- 예측 가능한 전체 배선 용이성 반영 (FanoutNet 방식)  

---

## ✅ 6. 실시간성과 품질을 동시에 얻는 이유

| 요소 | 설명 |
|------|------|
| **DQN** | 학습된 정책으로 즉각 결정 가능 → **실시간성 확보** |
| **MCTS** | 복잡한 탐색 문제에 대한 **강력한 완성도** |
| **PPO** | MCTS 탐색의 질을 향상시킴 → **성공률 보장 + 빠른 수렴** |
| **혼합 구조** | 빠르고, 정교하고, 실패하지 않는 구조 확보 |

---

## ✅ 결론: 최선의 배선 알고리즘 구조

```text
그래프 기반 상태 인코딩 +
DQN(실시간 추론) +
PPO(정책 학습) +
MCTS(탐색 완성도 보장)
→ 높은 배선 품질 + 성공률 + 속도
```

이 조합은 현재 논문과 실제 구현 사례들에서 모두 **가장 실용적이고 효과적인 방식**으로 확인되었습니다.

---

필요하시면 이 구조로 된 **시스템 아키텍처 그림**, **알고리즘별 비교표**, **학습 코드 뼈대** 등도 드릴 수 있어요.  
원하시나요? 😄
