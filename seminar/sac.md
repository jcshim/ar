# SAC(Soft Actor-Critic)

### TD3가 안정적인 학습을 위해 개선된 **"결정론적(Deterministic)"** 알고리즘이라면,  
### **SAC(Soft Actor-Critic)**는 한 단계 더 나아가서 **"확률적(Stochastic)"** 정책을 사용해 **더 유연하고 탐색적인** 학습을 가능하게 한 알고리즘

---

## 🎯 한 줄 요약: SAC란?
> **"탐험(exploration)을 잘하는 강화학습 알고리즘"**  
> 보상뿐 아니라 **행동의 다양성(무작위성)**도 고려해서 더 똑똑하게 학습함!

---

## 🔑 SAC의 핵심 아이디어 (3가지)

| 핵심 요소 | 설명 | 쉽게 말하면 |
|-----------|------|--------------|
| ✅ 확률적 정책 (Stochastic Policy) | 행동을 확률분포에서 샘플링 | 같은 상태에서도 다양한 행동 가능 |
| 🔥 엔트로피 보너스 (Entropy Bonus) | **무작위성 유지**를 위한 보상 항목 추가 | 너무 일찍 한 행동에만 집착하지 않게 함 |
| 🧠 Actor-Critic 구조 | 정책과 가치함수 별도로 학습 | TD3와 비슷하게 안정성 유지 |

---

## 📈 SAC vs TD3 비교

| 항목 | TD3 | SAC |
|------|-----|-----|
| 정책(Policy) | 결정론적 (정해진 행동) | **확률적** (행동을 뽑아냄) |
| 탐색 능력 | 중간 | **높음 (탐험 잘함)** |
| 보상 구조 | 보상만 고려 | **보상 + 엔트로피(무작위성)** 고려 |
| 안정성 | 높음 | 높음 (성능도 우수) |
| 사용 상황 | 안정성이 중요할 때 | **복잡하거나 일반화가 중요한 문제**에서 효과적 |

---

## 🔧 SAC 작동 순서 (쉽게 설명)

1. **상태 → 확률 분포 → 행동 선택**  
   같은 상태여도 행동이 조금씩 달라질 수 있어요. (탐험!)

2. **보상 + 무작위성(Entropy)** 고려해서 학습  
   무작위성이 높을수록 보상을 더 주는 구조로 처음엔 다양한 행동을 해보게 유도합니다.

3. **Q 네트워크 2개 사용**  
   TD3처럼 보수적인 Q값을 만들기 위해 두 개 사용.

4. **타깃 네트워크로 안정적 학습**  
   파라미터가 급격히 바뀌지 않도록 서서히 이동.

---

## 📌 왜 엔트로피를 보상에 넣을까?

SAC의 보상 식은 이렇게 생겼습니다:

\[
\text{최적화 목표} = \text{보상} + \alpha \cdot \text{엔트로피}
\]

- `보상`: 일반적인 환경 보상 (ex. 배선 길이 줄이기)
- `엔트로피`: 행동의 무작위성 (다양한 행동 해보기)
- `α`: 탐험의 중요도를 조절하는 계수 (하이퍼파라미터)

> 즉, **보상만 쫓는 게 아니라**, "다양한 행동을 해보는 것 자체도 좋다"고 학습시킴.

---

## 🧠 SAC의 장점

- ✅ **탐색과 수렴의 균형**이 뛰어남
- ✅ **복잡한 연속 공간**에서 우수한 성능
- ✅ 다양한 상황에서 **일반화 성능**이 좋음
- ✅ **엔트로피 조절(α)**로 전략 다양성을 조정 가능

---

## 🔬 왜 이 논문에서 SAC를 썼을까?

- **PCB 부품 배치 문제**는 위치가 연속적이고, 배치 가능한 경우가 엄청 많음.
- 초기에는 다양한 배치를 시도해봐야 하므로 **SAC처럼 탐험을 잘하는 알고리즘**이 적합.
- 엔트로피 기반 탐색 덕분에 **더 좋은 배치 방식에 도달할 확률이 높음**.

---

## 📌 그림으로 비교해보기

| TD3 | SAC |
|-----|-----|
| 같은 상태면 항상 같은 행동 | 같은 상태라도 행동이 매번 다를 수 있음 |
| 빠르게 수렴할 수 있음 | 더 다양한 전략을 시도함 |
| 안전한 보수적 전략 | 창의적이고 유연한 전략 |

---

## 📂 실제 코드로 요약 (PyTorch-style 의사코드)

```python
# Actor: 확률적 정책 (mean, std → action sample)
action = Normal(mean, std).sample()

# Critic: Q1, Q2 두 개의 가치함수
q1 = Q1(state, action)
q2 = Q2(state, action)
q_target = reward + gamma * (min(q1_target, q2_target) - alpha * log_prob)

# 엔트로피 조절: alpha 튜닝 가능
```

---

필요하시면:
- **TD3 vs SAC 성능 비교 실험**
- **PyTorch 코드 예시**
- **보상 함수 설계에서의 차이점**

같은 것도 자세히 설명드릴 수 있어요. 어떤 부분을 더 알고 싶으신가요? 😊
