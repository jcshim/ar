### 3월30일 일요일 오후5시에 특별세미나
1. 강사: University of Malta, Luke Vassallo
2. PCB Auto Routing 전문가이며, 석사논문 저자
3. GPT 활용해서 예습 부탁드려요. 1인당 하나씩 질문하기.

---

### [Source Code](https://github.com/LukeVassallo/RL_PCB)
### [석사논문](https://www.lukevassallo.com/wp-content/uploads/2023/09/automated_pcb_component_placement_using_rl_msc_thesis_v2_1_lv.pdf)

---

## **"Automated PCB Component Placement using Reinforcement Learning"**

---

### 📌 논문 주제 요약
이 논문은 **강화학습(Reinforcement Learning, RL)**을 활용해 **PCB(인쇄회로기판) 부품 자동 배치** 문제를 해결하려는 연구입니다.

---

### 1. 🔍 왜 이 연구를 했는가? (연구의 동기)
- PCB 설계에서 부품 배치는 매우 중요한 단계지만, 아직도 수작업에 의존하는 경우가 많습니다.
- 기존 자동화 도구는 직관이 부족하고, 사람처럼 "센스 있게" 배치하기 어려움.
- 그래서 **AI 기술**, 특히 **강화학습**을 사용해서 자동으로 부품을 배치할 수 있는 시스템을 만들고자 함.

---

### 2. 🎯 목표
1. **부품 배치 문제를 RL로 해결**하는 방식 제안.
2. **단일 부품**을 반복적으로 움직이면서 학습하는 환경 설계.
3. **다수의 부품**을 동시에 고려하는 학습 시스템 개발.
4. 기존 방법(Simulated Annealing)과 성능 비교.

---

### 3. 🛠️ 방법
- **MDP(Markov Decision Process)**로 배치 문제를 모델링.
- 강화학습 알고리즘인 **TD3, SAC**를 사용하여 학습.
- **보상 함수**를 조절해가며 학습 성능 향상 시도.
- 부품 간 협업/경쟁하는 행동이 학습되도록 설계.
- 실험에 사용할 데이터셋 30개를 실제 회로로부터 수집.

---

### 4. ✅ 주요 성과
- 강화학습이 기존 기법(SA)보다 **최대 21% 더 짧은 배선 길이**를 만들어냄.
- 적응형 보상 시스템 덕분에 **전문가의 지식 없이도 학습 가능**.
- 훈련된 정책(policy)이 보지 못한 새로운 회로에서도 잘 작동.
- **오픈소스**로 코드와 도구를 공개:  
  👉 [https://github.com/lukevassallo/rl_pcb](https://github.com/lukevassallo/rl_pcb)

---

### 5. 📌 결론
- 강화학습은 PCB 배치 자동화에 매우 유망한 방법임을 보여줌.
- 부품이 협력하며 자연스럽게 배치되는 방식까지 학습 가능.
- 향후 연구는 **오프라인 학습**이나 **보상 함수 개선**, **더 복잡한 회로로의 확장** 등이 있음.


# 🧠 논문 핵심 요약 (조금 더 자세히)

### 🔧 연구 목적
PCB(인쇄회로기판) 설계에서 **부품 배치(Component Placement)**는 설계 품질에 큰 영향을 미칩니다. 하지만 여전히 수작업에 많이 의존하고 있어 비효율적입니다.  
> 그래서 이 논문은 강화학습(RL)을 통해 **부품을 자동으로, 더 잘 배치**할 수 있는 시스템을 만들고자 했습니다.

### 🧩 문제 정의
- 부품 배치 문제는 **NP-Complete** 문제로, 최적해를 찾는 데 계산량이 매우 큼.
- 기존 방식은 시뮬레이티드 어닐링(SA)이나 유전 알고리즘 등을 사용하지만,
  - 직관 부족
  - 확장성 낮음
  - 전문가 개입 필요
- → 그래서 이 논문은 **AI가 스스로 배우면서** 좋은 배치를 하도록 하는 방향을 제안.

---

## 📐 접근 방법

### 구성
1. **Constructive Placement (초기 배치)**  
   회로 정보를 입력받아, 규칙 기반으로 부품을 하나씩 배치. 이 과정을 강화학습으로 개선 시도.
2. **Single-Component Iterative Placement (단일 부품 반복 학습)**  
   부품을 하나씩 선택해서 조금씩 위치를 바꾸며 더 나은 배치를 학습.
3. **Multi-Component Iterative Placement (다중 부품 동시 학습)**  
   여러 부품이 **협력 또는 경쟁**하면서 최적의 배치를 스스로 찾아가는 환경 설계.

### 강화학습 알고리즘 사용
- **TD3 (Twin Delayed DDPG)**  
- **SAC (Soft Actor-Critic)**  
이 두 알고리즘은 **연속적인 행동 공간**에서 잘 작동함.

---

## 🧪 실험 결과

| 실험 조건 | 성능 비교 (배선 길이) |
|-----------|----------------------|
| 기존 방식 (SA) | 기준 성능 |
| TD3 기반 정책 | 평균 17% 개선 |
| SAC 기반 정책 | 평균 21% 개선 |

- 학습된 정책은 새로운 회로에서도 일반화된 성능을 보여줌.
- 부품 간 **공간 양보, 우선 배치, 중심 정렬** 등 사람처럼 행동하는 양상도 관찰됨.
- **빠르게 수렴하는 배치 정책**이 학습됨 (빠른 시간 안에 좋은 결과 도출).

---

## 📘 용어 정리표

| 용어 | 뜻 | 설명 |
|------|----|------|
| PCB (Printed Circuit Board) | 인쇄회로기판 | 전자 부품이 연결되는 기판 |
| 부품 배치 (Component Placement) | 회로 부품 위치 결정 | 부품을 어떻게, 어디에 놓을지를 결정하는 설계 단계 |
| 강화학습 (Reinforcement Learning, RL) | 보상을 기반으로 학습 | 시행착오를 통해 행동을 학습하는 AI 방식 |
| MDP (Markov Decision Process) | 마르코프 결정 과정 | 상태-행동-보상 구조를 갖춘 학습 모델 |
| Actor-Critic | RL 알고리즘 구조 | 정책(Actor)과 가치 함수(Critic)를 함께 학습 |
| TD3 | 강화학습 알고리즘 | DDPG의 변형으로 안정성 개선 |
| SAC | 강화학습 알고리즘 | 확률적 정책을 활용해 탐색 강화 |
| 보상 함수 (Reward Function) | 학습 성과 기준 | 배선 길이, 겹침 방지 등을 고려해 설계됨 |
| SA (Simulated Annealing) | 기존 배치 최적화 기법 | 무작위 탐색 기반의 전통적인 최적화 기법 |
| Wirelength | 배선 길이 | 회로 배선의 총 길이, 짧을수록 좋음 |
| Overlap | 부품 겹침 | 부품이 서로 겹치지 않도록 함 |
| Policy | 정책 | 주어진 상태에서 어떤 행동을 취할지 결정하는 함수 |

---

## 📁 오픈소스 코드
- 연구자는 해당 방법론을 깃허브에 공개했습니다.  
👉 [https://github.com/lukevassallo/rl_pcb](https://github.com/lukevassallo/rl_pcb)

---

## 🔮 결론 및 향후 과제
- 이 논문은 강화학습이 PCB 설계 자동화에 실질적인 성능 개선을 제공할 수 있음을 입증함.
- 전문가의 개입 없이도 학습 가능한 보상 설계는 실용성과 확장성 측면에서 매우 유망.
- 향후 과제:
  1. 더 복잡한 회로로 확장
  2. 전문가 설계 데이터 반영
  3. **Offline RL** 탐색
  4. 보상 함수 및 환경 설계 고도화


