# 액터-크리틱(Actor-Critic) 강화학습

## 💡 액터-크리틱(Actor-Critic) 강화학습이란?

> “행동은 내가 정하지만, 누가 옆에서 ‘잘했는지’ 평가해주면 더 잘 배울 수 있지 않을까?”

그래서 역할을 나눕니다:

- 🎭 **Actor**: "내가 뭘 할지 결정하는 역할" → 정책(Policy)
- 🧑‍⚖️ **Critic**: "그 행동이 얼마나 좋은지 평가하는 역할" → 가치 함수(Value)

즉,  
👉 **Actor는 행동하고**,  
👉 **Critic은 평가하고**,  
👉 **둘이 협력해서 학습**합니다.

---

### ✅ 예시로 이해하기

#### 🎯 예: 다트 던지는 선수와 코치
- **선수(Actor)**: 다트를 어떻게 던질지 결정
- **코치(Critic)**: “이번엔 중심에서 너무 벗어났어”, “좋았어!” 라고 평가

→ 선수는 코치 피드백을 듣고 점점 더 좋은 던지기 방법을 익힘  
→ **혼자 연습하는 것보다 훨씬 빠르고 효율적**이죠!

---

## 🔧 왜 이런 구조를 쓰나요?

정책 기반만 쓰면, 보상 결과만 보고 학습하기 때문에  
→ 느리거나, 불안정할 수 있어요 😵

그런데 Critic이 **즉시 피드백을 주면**  
→ Actor는 더 빠르고 정확하게 학습할 수 있어요 👍

---

## 📌 주요 구성 요약

| 역할 | 설명 |
|------|------|
| **Actor** | 현재 상태에서 어떤 행동을 할지 결정 |
| **Critic** | 그 행동이 얼마나 좋은지 예측 (Q값 또는 V값 계산) |
| 🧠 학습 방법 | Critic이 계산한 가치 정보를 가지고 Actor를 업데이트 |

---

## 🎓 대표 알고리즘

### 🔸 A2C / A3C
- Advantage 값을 사용해서 더 정밀하게 평가
- A3C는 병렬 학습을 통해 효율성 업!

### 🔸 DDPG (Deep Deterministic Policy Gradient)
- 연속 행동 공간에서 작동하는 Actor-Critic
- Actor는 결정론적(Deterministic) 행동 선택

### 🔸 TD3 (Twin Delayed DDPG)
- DDPG의 개선판
- Q함수를 **2개** 써서 안정성 향상

### 🔸 SAC (Soft Actor-Critic)
- **탐험까지 잘하는** 액터-크리틱
- 보상 + 엔트로피 최대화 → 유연하고 스마트한 정책

---

## 🧠 비유로 다시 정리!

> 📘 시험공부하는 학생(Actor)  
> 👨‍🏫 옆에서 “그 공부법 괜찮아!” 또는 “좀 별로였어…”라고 알려주는 친구(Critic)

→ 친구의 피드백 덕분에 더 빠르고 효율적으로 실력을 늘리는 학생이 되는 거죠!

---

## 📍특징 정리

| 항목 | 설명 |
|------|------|
| 🎯 목적 | 정책과 가치 함수를 동시에 학습 |
| 🧠 장점 | 학습 속도 빠르고, 연속 환경에 유리 |
| ⚙️ 활용 | 로봇 제어, 시뮬레이션, 자율주행 등 실제 문제에 많이 사용 |
| ⚠️ 주의 | Actor와 Critic이 균형 있게 학습되지 않으면 불안정할 수 있음 |

---

필요하시면  
- 각 알고리즘(TD3, SAC 등) 구조도  
- 코드 예제, 비교 차트, 그림  
도 준비해드릴 수 있어요!

다음으로 궁금하신 주제 있으신가요? 😄  
예: “PPO vs SAC 비교해줘” / “DQN 코드 보고 싶어” / “탐험 전략 좀 더 자세히” 등!
