# **정책 기반(Policy-based)** 강화학습
### 앞서 설명한 가치 기반이 "어떤 행동이 이득일까?"를 계산해서 고르는 방식이라면,  
정책 기반은 처음부터 "이 상태에서 뭘 할지 직접 정한다"는 접근이에요.

---

## 💡 정책 기반(Policy-based) 강화학습이란?

> “이 상태에서는 어떤 행동을 어느 확률로 할지 직접 정하자!”  
> = **정책(Policy)**를 바로 학습하는 방식

즉, 에이전트는  
**상태 → 행동 확률**을 출력하는 함수(=정책 함수, π)를 딥러닝으로 만들어서,  
**그 확률에 따라 행동을 선택**해요.

---

### ✅ 예시로 풀어볼게요:

#### 🎯 예: 다트 던지기 게임
- 상태(state): 바람이 센가? 조준은 잘 되어 있나?
- 행동(action):
  1. 약하게 던지기  
  2. 세게 던지기  
  3. 중간 세기로 던지기

➡️ 정책 기반 방법은 이렇게 행동의 **확률을 직접 출력**해요:  
- 약하게: 20%  
- 세게: 30%  
- 중간: 50%

그리고 **그 확률대로 무작위로 행동을 선택**해서 실행해보고,  
**잘되면 그 행동 확률을 더 높이고**,  
실패하면 낮추는 방향으로 **정책을 업데이트**합니다.

---

## 🔧 어떻게 학습하나요?

이건 수학적으로는 **정책 경사법(Policy Gradient)**을 이용해요.  
보상을 많이 받은 행동이 나오도록 정책의 파라미터를 조금씩 조정하죠.

> 📈 “이 행동이 점수를 많이 줬어!” → 그 방향으로 확률을 더 키우자

---

## 🧠 비유로 다시 한 번!

> 🎲 감으로 정답 찍기

- 처음엔 감으로 아무렇게나 찍어요 (확률적으로 행동 선택)  
- 찍은 게 정답이면 → “오, 이 방식 괜찮은데?” → **그 방식 더 자주 쓰게 학습**  
- 점점 더 높은 확률로 정답을 찍게 됨 = 정책이 점점 똑똑해짐

---

## 📌 특징 정리

| 항목 | 설명 |
|------|------|
| 🌟 무엇을 학습하나요? | 정책 함수 (π), 상태 → 행동 확률 |
| 🧠 정책은 어떻게 사용하나요? | 확률 분포에서 샘플링 (랜덤 선택) |
| 🔢 행동 공간 | **연속/이산 모두 가능** |
| 🚀 탐험 방식 | 확률적 선택 자체가 탐험을 유도 |
| ⚠️ 단점 | 불안정하거나 수렴이 느릴 수 있음 |

---

## 🎓 대표 알고리즘

### 1. **REINFORCE**
- 가장 기본적인 정책 경사법
- 한 에피소드 끝까지 보고, 보상 기준으로 정책 업데이트  
- 단점: 변동성 큼, 느림

### 2. **PPO (Proximal Policy Optimization)**
- 정책을 **조금씩, 안전하게 업데이트**
- 너무 급격하게 바뀌지 않도록 제한 → **안정성 개선**

### 3. **A2C / A3C**
- 액터-크리틱 구조 (정책 기반 + 가치 기반 혼합)
- Critic이 Actor에게 “이 행동 괜찮았어~” 하고 도와줌

---

## 📌 정리 요약

- **정책 기반**은 행동을 바로 선택하는 방법을 학습
- **확률적으로 행동을 고르기 때문에 탐험이 자연스러움**
- **연속적인 행동 공간**에서 특히 유리
- 안정성을 위해 PPO 같은 개선된 알고리즘이 등장
