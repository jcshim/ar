## ✅ 최적화 방식에 따른 **강화학습의 주요 종류**

여기서는 특히 **정책 최적화 관점**에서 쉽게 정리해볼게요.  
크게 3가지 방식이 있어요:

---

### 🔵 1. **가치 기반(Value-based)**  
: 상태(state)에서 어떤 행동(action)을 했을 때 **기대 보상(Q-value)**을 예측해서 학습

#### 대표 알고리즘:
- **Q-learning**
- **DQN (Deep Q-Network)**

#### 🔧 핵심:
> "지금 이 상황에서 어떤 행동이 제일 보상이 클까?"를 예측한 다음, **가장 큰 보상을 주는 행동을 선택**함.

#### 📌 특징:
- 보통 **이산(discrete)** 행동 공간에서 사용
- 정책(Policy)이 명시적으로 없고, Q함수로부터 유도

#### 🧠 비유:
> 시험에서 점수 잘 받을 문제를 예상해서 그 문제만 공부하는 학생

---

### 🔴 2. **정책 기반(Policy-based)**  
: 정책(Policy)을 직접 학습하여 **행동 확률을 바로 출력**하는 방식

#### 대표 알고리즘:
- **REINFORCE**
- **PPO**
- **A2C / A3C**

#### 🔧 핵심:
> “지금 이 상태에서는 이렇게 행동하자”  
→ 확률 분포로 바로 행동을 샘플링

#### 📌 특징:
- **연속적인 행동 공간**도 가능
- 탐험이 자연스럽고, 최적화는 확률분포의 gradient로 함
- 불안정할 수 있어서 **PPO, TRPO**처럼 개선된 방법이 나옴

#### 🧠 비유:
> 감으로 정답 찍되, 잘 찍은 건 더 자주 찍게 학습하는 학생

---

### 🟢 3. **액터-크리틱(Actor-Critic)**  
: 가치기반 + 정책기반의 **장점 합친 하이브리드 방식**

#### 대표 알고리즘:
- **DDPG**
- **TD3**
- **SAC**
- **A2C / A3C** (여기서도 등장)

#### 🔧 핵심:
- **Actor**: 정책을 담당 (행동 선택)
- **Critic**: 가치 함수 담당 (좋은 행동인지 판단)
- Critic이 Actor의 행동을 평가하며 함께 학습

#### 📌 특징:
- 효율적이고 안정적
- 연속 공간에 적합
- 최근 RL 연구에서 많이 사용됨

#### 🧠 비유:
> 내가 정답을 찍고, 친구가 “그 선택 괜찮았어!” 라고 피드백 줌  
→ 그걸 참고해서 더 나은 정답을 찍음

---

## 📚 전체 그림 정리

| 구분 | 방식 | 정책 학습 | 대표 알고리즘 | 특징 |
|------|------|------------|----------------|------|
| **1. 가치 기반** | Q함수 최적화 | 없음 (Q로부터 도출) | Q-learning, DQN | 간단, 이산공간에 적합 |
| **2. 정책 기반** | 정책 함수 직접 최적화 | 있음 | REINFORCE, PPO | 탐험 자연스러움, 불안정 가능 |
| **3. 액터-크리틱** | 정책 + 가치 함수 함께 최적화 | 있음 | DDPG, TD3, SAC | 안정적이고 효율적, 연속공간 가능 
